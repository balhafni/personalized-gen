{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates the data for the data scaling experiments. We are interested in answering the question: how much data per author do we need? We will create multiple versions of the data by looking at the cumulative number of words per author: 1000, 5000, 10000, 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from data_utils import Corpus\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        return [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_data(data):\n",
    "    data_by_author = defaultdict(list)\n",
    "    for example in data:\n",
    "        data_by_author[example['author']].append(example)\n",
    "    return data_by_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data_per_author(examples, word_limit=15000):\n",
    "    scaled_examples = list()\n",
    "    current_word_cnt = 0\n",
    "\n",
    "    for example in examples:\n",
    "        if current_word_cnt + len(example['text'].split()) <= word_limit:\n",
    "            scaled_examples.append({'author': example['author'],\n",
    "                                    'file': example['title'],\n",
    "                                    'text': example['text'],\n",
    "                                    'domain': example['doc_atts']['domain']})\n",
    "            current_word_cnt += len(example['text'].split())\n",
    "\n",
    "        else:\n",
    "            # if we reach a document that's too long, we will keep adding \n",
    "            # sentences until we reach the word limit\n",
    "            annotated_example = Corpus(author=example['author'],\n",
    "                                       title=example['title'],\n",
    "                                       domain=example['doc_atts']['domain'],\n",
    "                                       text=example['text']\n",
    "                                    )\n",
    "\n",
    "            split_example = []\n",
    "            for sent in annotated_example.sents:\n",
    "                if current_word_cnt + len(sent.split()) <= word_limit:\n",
    "                    split_example.append(sent)\n",
    "                    current_word_cnt += len(sent.split())\n",
    "                else:\n",
    "                    scaled_examples.append({'author': example['author'],\n",
    "                                            'file': example['title'],\n",
    "                                            'text': \" \".join(split_example),\n",
    "                                            'domain': example['doc_atts']['domain']})\n",
    "                    return scaled_examples\n",
    "\n",
    "    return scaled_examples\n",
    "\n",
    "\n",
    "def scale_data(data, limit):\n",
    "    scaled_data = dict()\n",
    "\n",
    "    for author in data:\n",
    "        examples = data[author]\n",
    "        scaled_examples = scale_data_per_author(examples, word_limit=limit)\n",
    "        scaled_data[author] = scaled_examples\n",
    "\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_num_words(data):\n",
    "    num_words = 0\n",
    "\n",
    "    for author in data:\n",
    "        for example in data[author]:\n",
    "            num_words += len(example['text'].split())\n",
    "\n",
    "    return num_words / len(data)\n",
    "\n",
    "\n",
    "def stats(data):\n",
    "    # authors\n",
    "    num_authors = len(data)\n",
    "    # docs\n",
    "    num_docs = sum([len(x) for v, x in data.items()])\n",
    "    # avg # docs per author\n",
    "    avg_num_docs = num_docs / len(data)\n",
    "    # avg # words per doc\n",
    "    total_words = 0\n",
    "    for author, docs in data.items():\n",
    "        total_words += sum([len(x['text'].split()) for x in docs])\n",
    "\n",
    "    avg_words_per_doc = total_words / num_docs\n",
    "\n",
    "    # avg # per author\n",
    "    avg_words_per_author = total_words / len(data)\n",
    "\n",
    "    print(f'Num Authors: {len(data)}')\n",
    "    print(f'Num Docs: {num_docs}')\n",
    "    print(f'Avg Num Docs: {avg_num_docs}')\n",
    "    print(f'Avg Words Per Doc: {avg_words_per_doc}')\n",
    "    print(f'Avg Words Per Author: {avg_words_per_author}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(data, path):\n",
    "    with open(path, mode='w') as f:\n",
    "        for author, docs in data.items():\n",
    "            for doc in docs:\n",
    "                f.write(json.dumps(doc))\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('../data/train.json')\n",
    "data_by_author = group_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs = dict()\n",
    "imdb = dict()\n",
    "amazon = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in data_by_author:\n",
    "    domain = data_by_author[author][0]['doc_atts']['domain']\n",
    "\n",
    "    if domain == 'blog':\n",
    "        blogs[author] = data_by_author[author]\n",
    "\n",
    "    elif domain == 'retail':\n",
    "        amazon[author] = data_by_author[author]\n",
    "\n",
    "    elif domain == 'movie':\n",
    "        imdb[author] = data_by_author[author]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Authors: 140\n",
      "Num Docs: 19879\n",
      "Avg Num Docs: 141.99285714285713\n",
      "Avg Words Per Doc: 214.94426278987876\n",
      "Avg Words Per Author: 30520.55\n",
      "\n",
      "Num Authors: 62\n",
      "Num Docs: 30933\n",
      "Avg Num Docs: 498.9193548387097\n",
      "Avg Words Per Doc: 253.443054343258\n",
      "Avg Words Per Author: 126447.64516129032\n",
      "\n",
      "Num Authors: 49\n",
      "Num Docs: 34012\n",
      "Avg Num Docs: 694.1224489795918\n",
      "Avg Words Per Doc: 183.11531224273784\n",
      "Avg Words Per Author: 127104.44897959183\n"
     ]
    }
   ],
   "source": [
    "stats(blogs)\n",
    "print()\n",
    "stats(imdb)\n",
    "print()\n",
    "stats(amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_by_author = group_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_1k = scale_data(train_data_by_author, limit=1000)\n",
    "scaled_data_5k = scale_data(train_data_by_author, limit=5000)\n",
    "scaled_data_10k = scale_data(train_data_by_author, limit=10000)\n",
    "scaled_data_20k = scale_data(train_data_by_author, limit=20000)\n",
    "scaled_data_35k = scale_data(train_data_by_author, limit=35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(987.8286852589641,\n",
       " 4987.673306772908,\n",
       " 9962.940239043824,\n",
       " 18871.211155378485,\n",
       " 28895.37450199203)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_num_words(scaled_data_1k), avg_num_words(scaled_data_5k), avg_num_words(scaled_data_10k), avg_num_words(scaled_data_20k), avg_num_words(scaled_data_35k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write(scaled_data_1k, path='../data/scaling_exps/1k/train_1k.raw.json')\n",
    "write(scaled_data_5k, path='../data/scaling_exps/5k/train_5k.raw.json')\n",
    "write(scaled_data_10k, path='../data/scaling_exps/10k/train_10k.raw.json')\n",
    "write(scaled_data_20k, path='../data/scaling_exps/20k/train_20k.raw.json')\n",
    "write(scaled_data_35k, path='../data/scaling_exps/35k/train_35k.raw.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
